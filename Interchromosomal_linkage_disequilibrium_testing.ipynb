{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Dan Shea  \n",
    "Date: 2019.08.15\n",
    "\n",
    "# Inter-chromosomal Linkage Disequilibrium testing of common SNPs\n",
    "\n",
    "Next, we will compare the set of SNPs common to all 20 founders (3,745 SNPs) by partitioning the genome (by chromosome) into $\\mathscr{K}$ partitions. In rice this yields $| \\mathscr{K} | = 12$. All of the SNPs $\\mathscr{s}_{i} \\in \\mathscr{k}_{1}$ are then compared to all SNPs $\\mathscr{s}_{j} \\in {\\mathscr{K} - \\mathscr{k}_{1}}$. This yields all $\\langle \\mathscr{s}_{i}, \\mathscr{s}_{j} \\rangle$ pairs of SNPs. Since order is not important, we can proceed to the next chromosome $\\mathscr{k}_2$ and compare SNPs $\\mathscr{s}_{i}$ with $\\mathscr{s}_{j} \\in {\\mathscr{K} - \\mathscr{k}_{1} - \\mathscr{k}_{2}$. Once we have done this for the first 11 chromosomes, we have generated all possible SNP combinations between SNPs on different chromosomes.\n",
    "\n",
    "In the absence of _Linkage Disequilibrium_ (LD), we can expect a single SNP has $P(x) = 0.5$. Since we are looking at an _independent_ event at each position in our dimer (i.e. - the $\\langle \\mathscr{s}_{i}, \\mathscr{s}_{j} \\rangle$ pair) the _expected frequency of observation_ ($E(f_{obs})$) for any given dimer is simply $P(x,y) = P(x) \\cdot P(y) = 0.5 \\cdot 0.5 = 0.25$\n",
    "\n",
    "We have an expected $f_{obs}$ and our actual $f^{\\prime}_{obs}$, so we may perform a $\\chi^{2}$ goodness-of-fit test to examine if the genotypes of the dimers observed is in line with the expectation that there is no LD between the two loci. (This expectation of no LD between loci on two different chromosomes being our $H_{0}$ for the $\\chi^{2}$ test.)\n",
    "\n",
    "Given that our SNP set is comprised of 3,745 loci and we want to examine pairs of SNPs, comparing every SNP to every other SNP would combinatorically be $C_{k}(n) = \\frac{n!}{k!\\cdot(n-k)!} = C_{2}(3,745) = \\frac{3,745 \\cdot 3,744}{2} = 7,010,640$. However, our actual number of comparisons will be smaller than this, because we further restrict pairs to be on different chromosomes.\n",
    "\n",
    "After we complete our tests and receive p-values, we then FDR correct them using `statsmodels.stats` `multitest.fdrcorrection()` function. The resulting FDR-corrected p-values (i.e. - q-values) are then compared to $\\alpha = 0.05$ and deemed significant if $q<\\alpha$.\n",
    "\n",
    "Inter-chromosomal Linkage Disequilibrium can provide evidence of epistatic interaction. _Epistasis_ is the effect of one gene's genotype influencing the expression of another gene.\n",
    "\n",
    "OK, enough of me blathering, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from statsmodels.stats import multitest\n",
    "import os\n",
    "import os.path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['N01','N03','N04','N05','N06','N07','N08','N09','N10','N11',\n",
    "           'N12','N13','N14','N16','N17','N18','N19','N20','N21','N22',]\n",
    "founders = ['KASALATH','KEIBOBA','SHONI','TUPA_121-3','SURJAMUKHI','RATUL','BADARI_DHAN','KALUHEENATI','JAGUARY','REXMONT',\n",
    "            'URASAN','TUPA_729','DEE_JIAO_HUA_LUO','NERICA_1','TAKANARI','C8005','MOUKOTOU','NORTAI','SESIA','HAYAYUKI',]\n",
    "datadirs = ['_'.join([x, y]) for x, y in zip(samples, founders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele_frequency_files = [os.path.join('beagle_output', x, y+'_allele_frequencies.tsv') for x, y in zip(datadirs, samples)]\n",
    "genotype_files = [os.path.join('beagle_output', x, y+'_genotypes.tsv') for x, y in zip(datadirs, samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dshea/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "frequency_dfs = OrderedDict()\n",
    "for key, value in zip(samples, allele_frequency_files):\n",
    "    frequency_dfs[key] = pd.read_csv(value, sep='\\t', index_col=0)\n",
    "    \n",
    "genotype_dfs = OrderedDict()\n",
    "for key, value in zip(samples, genotype_files):\n",
    "    genotype_dfs[key] = pd.read_csv(value, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs = OrderedDict()\n",
    "for key in samples:\n",
    "    data_dfs[key] = pd.concat([genotype_dfs[key], frequency_dfs[key]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer require these dfs, so let's delete them to free up some memory\n",
    "del(frequency_dfs)\n",
    "del(genotype_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the merged data that forms the 3,745 common SNPs\n",
    "merged_data = pd.read_csv('beagle_output/Common_SNPS_all_founders.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need the common loci, we extract the other information from our previously constructed data_dfs\n",
    "merged_data = merged_data.loc[:, ['CHROM', 'POS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_dfs = OrderedDict()\n",
    "for key in samples:\n",
    "    filtered_data_dfs[key] = pd.merge(merged_data, data_dfs[key], how='inner', on=['CHROM', 'POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N01 is (3745, 188)\n",
      "N03 is (3745, 94)\n",
      "N04 is (3745, 142)\n",
      "N05 is (3745, 219)\n",
      "N06 is (3745, 95)\n",
      "N07 is (3745, 81)\n",
      "N08 is (3745, 96)\n",
      "N09 is (3745, 264)\n",
      "N10 is (3745, 163)\n",
      "N11 is (3745, 204)\n",
      "N12 is (3745, 158)\n",
      "N13 is (3745, 104)\n",
      "N14 is (3745, 44)\n",
      "N16 is (3745, 55)\n",
      "N17 is (3745, 156)\n",
      "N18 is (3745, 267)\n",
      "N19 is (3745, 266)\n",
      "N20 is (3745, 261)\n",
      "N21 is (3745, 267)\n",
      "N22 is (3745, 260)\n"
     ]
    }
   ],
   "source": [
    "# We quickly sanity check this by ensuring the shapes are all 3,745 rows long\n",
    "for key in filtered_data_dfs.keys():\n",
    "    print('{} is {}'.format(key, filtered_data_dfs[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of strings for all chromosome ids\n",
    "chroms = [''.join(['chr','{0:02d}'.format(n)]) for n in range(1, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    s_df = filtered_data_dfs[key].copy()\n",
    "    filtered_data_dfs[key] = OrderedDict()\n",
    "    for c in chroms:\n",
    "        mask = s_df.CHROM == c\n",
    "        filtered_data_dfs[key][c] = s_df.loc[mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    for c in chroms:\n",
    "        filtered_data_dfs[key][c] = pd.concat([filtered_data_dfs[key][c].loc[:,['CHROM', 'POS']],\n",
    "                                               filtered_data_dfs[key][c].iloc[:, 11:-6]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need the data_dfs, so let's free up some memory\n",
    "del(data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OrderedDict()\n",
    "for key in samples:\n",
    "    results[key] = list()\n",
    "    c = chroms[:]\n",
    "    while c:\n",
    "        current_c = c.pop(0)\n",
    "        for other_c in c:\n",
    "            for row_i in range(0, filtered_data_dfs[key][current_c].shape[0]):\n",
    "                for row_j in range(0, filtered_data_dfs[key][other_c].shape[0]):\n",
    "                    pos_i = filtered_data_dfs[key][current_c].iloc[row_i ,1]\n",
    "                    pos_j = filtered_data_dfs[key][other_c].iloc[row_j ,1]\n",
    "                    dimers = filtered_data_dfs[key][current_c].iloc[row_i, 2:].to_numpy() + \\\n",
    "                             filtered_data_dfs[key][other_c].iloc[row_j, 2:].to_numpy()\n",
    "                    f_obs = [0, 0, 0, 0]  # index 0=AA, 1=AB, 2=BA, 3=BB\n",
    "                    for gt in dimers:\n",
    "                        if gt == 'AA':\n",
    "                            f_obs[0] += 1\n",
    "                        elif gt == 'AB':\n",
    "                            f_obs[1] += 1\n",
    "                        elif gt == 'BA':\n",
    "                            f_obs[2] += 1\n",
    "                        elif gt == 'BB':\n",
    "                            f_obs[3] += 1\n",
    "                    f_exp = np.array([0.25, 0.25, 0.25, 0.25]) * len(dimers)\n",
    "                    chisquare_val, pvalue = stats.chisquare(f_obs, f_exp)\n",
    "                    results[key].append([current_c, pos_i, other_c, pos_j,\n",
    "                                                    f_obs[0], f_obs[1], f_obs[2], f_obs[3],\n",
    "                                                   chisquare_val, pvalue])\n",
    "    # Now we've done all comparisons for a given founder\n",
    "    # We can construct the dataframe for the results and perform FDR correction on the pvalues\n",
    "    results[key] = pd.DataFrame(data=results[key], columns=['CHROM_a', 'POS_a', 'CHROM_b', 'POS_b',\n",
    "                                                            'AA_obs', 'AB_obs', 'BA_obs', 'BB_obs',\n",
    "                                                            'chisquare', 'pvalue'])\n",
    "    significant, qvalue = multitest.fdrcorrection(results[key].loc[:, 'pvalue'].to_numpy())\n",
    "    results[key]['qvalue'] = qvalue\n",
    "    results[key]['significant'] = significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, f in zip(samples, founders):\n",
    "    results[key].to_csv('interchromosomal_linkage_analysis/{}_{}_interchromosomal_ld.tsv'.format(key, f),\n",
    "                        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retrieve significant LD results\n",
    "significant_results = OrderedDict()\n",
    "for key in samples:\n",
    "    significant_results[key] = results[key].loc[results[key].significant == True, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to obtain centroids for all loci showing interchromosomal LD\n",
    "max_distance = 100000\n",
    "centroids = OrderedDict()\n",
    "for key in samples:\n",
    "    rows = significant_results[key].itertuples(index=False)\n",
    "    # Prime zee pump!\n",
    "    try:\n",
    "        prev_row = next(rows)\n",
    "    except StopIteration:\n",
    "        # Iterator was empty so we call continue to move to the next sample\n",
    "        continue\n",
    "    # OK if we got here this sample should get a key in the OrderedDict()\n",
    "    centroids[key] = list()\n",
    "    start_pos = prev_row\n",
    "    end_pos = prev_row\n",
    "    for curr_row in rows:\n",
    "        if (curr_row.CHROM_a == prev_row.CHROM_a) and (curr_row.CHROM_b == prev_row.CHROM_b):\n",
    "            if (curr_row.POS_a - prev_row.POS_a <= max_distance) and (curr_row.POS_b - prev_row.POS_b <= max_distance):\n",
    "                # Expandificate your locus to include this marker\n",
    "                end_pos = curr_row\n",
    "            else:\n",
    "                # We just done entered a new locus dudebro!\n",
    "                # Add the centroid-pair for the locus to the list of centroids for the sample\n",
    "                centroids[key].append([start_pos.CHROM_a, np.mean([start_pos.POS_a, end_pos.POS_a]),\n",
    "                                       start_pos.CHROM_b, np.mean([start_pos.POS_b, end_pos.POS_b])])\n",
    "                # Now reset the start_pos & end_pos, because we are at a new locus\n",
    "                start_pos = curr_row\n",
    "                end_pos = curr_row\n",
    "        else:\n",
    "            # Walked into a new chromosome, add the centroid-pair for the locus to the list of centroids for the sample\n",
    "            centroids[key].append([start_pos.CHROM_a, np.mean([start_pos.POS_a, end_pos.POS_a]),\n",
    "                                   start_pos.CHROM_b, np.mean([start_pos.POS_b, end_pos.POS_b])])\n",
    "            # Now reset the start & end_pos, because we are at a new locus\n",
    "            start_pos = curr_row\n",
    "            end_pos = curr_row\n",
    "        # No matter what else happened up above, the curr_row is now the prev_row\n",
    "        prev_row = curr_row\n",
    "    # Now we're done with this sample, so let's make this list of lists into a DataFrame\n",
    "    centroids[key] = pd.DataFrame(data=centroids[key], columns=['CHROM_a', 'POS_a', 'CHROM_b', 'POS_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['N01', 'N03', 'N04', 'N05', 'N06', 'N07', 'N08', 'N09', 'N10', 'N11', 'N12', 'N14', 'N16', 'N17', 'N18', 'N20', 'N21', 'N22'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM_a</th>\n",
       "      <th>POS_a</th>\n",
       "      <th>CHROM_b</th>\n",
       "      <th>POS_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr01</td>\n",
       "      <td>6604804.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>27955515.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr01</td>\n",
       "      <td>6978969.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>32668666.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>4418395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>7243744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>7598355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>7814911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>10712624.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>10873916.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>11087445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr01</td>\n",
       "      <td>8036400.0</td>\n",
       "      <td>chr02</td>\n",
       "      <td>11449712.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM_a      POS_a CHROM_b       POS_b\n",
       "0   chr01  6604804.0   chr02  27955515.5\n",
       "1   chr01  6978969.0   chr02  32668666.5\n",
       "2   chr01  8036400.0   chr02   4418395.0\n",
       "3   chr01  8036400.0   chr02   7243744.0\n",
       "4   chr01  8036400.0   chr02   7598355.0\n",
       "5   chr01  8036400.0   chr02   7814911.0\n",
       "6   chr01  8036400.0   chr02  10712624.5\n",
       "7   chr01  8036400.0   chr02  10873916.5\n",
       "8   chr01  8036400.0   chr02  11087445.0\n",
       "9   chr01  8036400.0   chr02  11449712.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids['N01'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in centroids.keys():\n",
    "    centroids[key].to_csv('interchromosomal_linkage_analysis/{}_ICLD_centroids.tsv'.format(key), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
